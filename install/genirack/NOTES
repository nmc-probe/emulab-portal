Setting up an InstaGeni Rack. First, we need the following info:

* We start here, after you have sent Utah "checklist", waiting for the
  images to be baked. Once you hear back from us, you may continue
  with these instructions.

  Waiting, waiting, waiting ...

* Power on the control switch (Procurve 2620 in the top slot).

* Attach a console to the control node and power on the node. You will
  wait a while for the "HP ProLiant" screen. Watch CAREFULLY, looking
  for the moment it says "press any key for Option ROM messages."
  Press any key!

  and then right after the screen
  switches, type F8 to get into the iLo configuration. Gotta be fast
  on this. If you miss it, power cycle. 

* Once the iLo screen comes up, right arrow to Network and choose the
  DHCP option. You want to make sure DHCP is off. F10 to save and then
  esc to go back. Then choose the NIC option. Fill in the iLo
  IP/Mask/Router, then F10 to save and then esc to exit. iLo will
  reset.

* Is the RAID array setup? It wasn't on Utah's rack. Need to fill this
  part in.

* At this point, Utah can do the rest of the rack setup without
  further intervention from you. Well, unless something gets wedged
  and Utah needs something to be physically power cycled. Or you can
  go back to your desk and proceed to setup the rack using the
  following instructions. Which do you prefer? I know you will make
  the correct choice.

* Now that you have decided, send email to Utah asking us to complete
  the installation while you go back to working on other projects. 

======================================================================

* Using your web browser, go to the iLo IP you set, and login using
  Administrator and the iLo password that is stamped on top of the
  control node, or in the data file you received. If using the
  datafile, look for the section that says:

	<u_location>U34</u_location>

  cause the control node is slot 34. Grab the lo_passwd from that
  section; that is you iLo password.

* In a shell window, you want to ssh over to the iLo:

	boss> ssh Administrator@iloIP

  This can be slow, so be patient. Once you get logged in, enter
  "textcons" at the command prompt. This will put you into a wacky
  text representation of the graphic console. To exit from the
  text console, use ESC-(

* Go to the Virtual Media tab, and then on the right hand side specify
  the url of the boot ISO image. This will be something like:
  
	http://155.98.32.70/downloads/genirack.iso
	
  which is Utah's web server. Check the box to boot from the CD on the
  next reset. The click on "Insert Media", and then after you get the
  confirmation that it attached okay, click the reset button.
  
* Wait for the node to boot. It should boot from the virtual CD drive
  since there is no other boot media, but if not you can hit F11 on
  the next go around, which will give you a list of options. Type
  whatever number is to the left of the CD choice.  

* The ISO will load and give you a boot prompt. This takes a while.
  You will eventually get a shell prompt after a lot of noisy
  output. This can take several minutes since it is demand loading the
  "CD" from Utah's web server. Be patient.

* Fire up the network:

	ifconfig eth0 inet Control_IP netmask Control_Mask
	route add default gw Gateway_IP

  The Control_IP is *NOT* the iLo IP you used above. It is the IP you
  have assigned to the control node itself.

* Transfer the control node image from Utah:

	cd /tmp
	wget http://155.98.32.70/downloads/genirack-1.ndz

  This is about a 1GB so it will take a while.

* Write the image file to the disk using the Emulab decompression tool:

	/usr/bin/imageunzip -o genirack-1.ndz /dev/cciss/c0d0

  This will take a little while. Watch the dots. There is a pause at
  the end while buffers are flushed to disk. Be patient. 

* Set the boot order so that the control node does not try to boot
  from the network, unless all else fails.

	cd /TOOLKIT
	./setbootorder floppy cdrom usb hd pxe

* Type "reboot" at the shell prompt. With any luck, the node will boot
  first time and you can ssh into the control node as elabman. You
  will need to add the key from /root/.ssh/elabman_dsa to your ssh agent,
  and the pass phrase is in boss:/usr/testbed/etc/elabman_dsa.pswd


  * Make sure all five of the experimental nodes are fully powered off;
  the ilo has to be off, and the easiest thing to do is just unplug
  them.

  Also make sure that the serial cable is connected to the 2620.

* Connect to the 2620 using this command:

	sudo screen /dev/ttyS0 19200

  It might not do anything when you carriage return; it is trying to sync
  up the speed (the switch does auto-sense). Wait 30 seconds, hit carriage
  return a few times again. If still not working, exit from screen ("^A \")
  and try again. Might take another iteration or two. When you have the
  prompt: 

  2610> config
  2610(config)> vlan 11
  2610(vlan-11)> name control-alternate
  2610(vlan-11)> untagged 24        XXXX Make sure about port number!
  2610(vlan-11)> ip address 10.2.1.253/24
  2610(vlan-11)> exit
  2610(config)> vlan 10
  2610(vlan-10)> name control-hardware
  2610(vlan-10)> untagged 23	   XXXX Make sure about port number!
  2610(vlan-10)> ip address 10.1.1.253/24
  2610(vlan-10)> exit
  2610(config)> vlan 1
  2610(vlan-1)> ip address 10.254.254.253/24  # IGMP querier requires this
  2610(vlan-1)> exit
  2610(config)> management-vlan 10
  2610(config)> ip default-gateway 10.1.1.254
  2610(config)> vlan 1 ip igmp
  2610(config)> vlan 1 ip igmp querier
  2610(config)> no web-management
  2610(config)> no snmp-server community public
  2610(config)> snmp-server community XXXXX manager unrestricted
  2610(config)> password all (type in same password for manager/operator)
  2610(config)> write memory
  2610(config)> reload

  UTAH: THE PASSWORD and COMMUNITY string come from variables.txt file
  in the rack subdir. Use the same for both switches.

  The switch will take moment to reset so you might lose your connection
  to the control node.

  Ping 10.1.1.253 and 10.2.1.253 to make sure things worked okay.
  Then telnet to 10.1.1.253 and make sure you can login using the
  switch password.

* Have the local site admin move the console cable to the 5406.
  Wait, wait, wait. 

* Connect to the 5406 using this command:

	sudo screen /dev/ttyS0 115200

  It might not do anything when you carriage return; it is trying to sync
  up the speed (the switch does auto-sense). Wait 30 seconds, hit carriage
  return a few times again. If still not working, exit from screen ("^A \")
  and try again. Might take another iteration or two. When you have the
  prompt: 
  
  5400> config
  5400(config)> no vlan 1 ip address
  5400(config)> vlan 10
  5400(vlan-10)> name control-hardware
  5400(vlan-10)> untagged A20	  XXXX Make sure about port number! E20?
  5400(vlan-10)> ip address 10.3.1.253/24
  5400(vlan-10)> exit 
  5400(config)> management-vlan 10
  5400(config)> ip default-gateway 10.3.1.254
  5400(config)> no web-management
  5400(config)> no snmp-server community public
  5400(config)> snmp-server community XXXXX manager unrestricted
  5400(config)> password all (type in same password for manager/operator)
  5400(config)> write memory
  5400(config)> reload
  
  USE THE SAME PASSWORD and COMMUNITY XXXXX as above (2620)

  Wait for the switch to reboot and confirm you can telnet to 10.3.1.253
  and log in using the switch password. 
  
* Create the 4th partition in the partition table:

	sudo fdisk /dev/sda

  Use the "n" option, primary partition type, default start, +950G, "w"
  to write it out.

  Then inform the kernel:

	sudo partprobe -s

* Initialize the LVM partition. We use LVMs for the boss/ops filesystems.

	sudo pvcreate /dev/sda4
	sudo vgcreate xen-vg /dev/sda4
    	sudo vgchange -a y xen-vg

* Create a filesystem to hold the boss/ops tarballs. These are pretty
  big but will be deleted after we copy the filesystems into their own
  lvms.

	sudo mkdir /scratch
	sudo /sbin/lvcreate -n scratch -L 75G xen-vg
	sudo mke2fs -j /dev/xen-vg/scratch
	sudo mount /dev/xen-vg/scratch /scratch
	sudo chmod 777 /scratch

* Copy the boss/ops tarfile to /scratch on the control node, and
  then unpack it. There will be two directories, ops and boss.

* Restore the VMs:

        mkdir ~elabman/boss ~elabman/ops
	sudo ~elabman/restorevm.pl -t ~elabman/boss boss /scratch/boss
	sudo ~elabman/restorevm.pl -t ~elabman/ops  ops /scratch/ops

  This creates a bunch of LVMs and rewrites the xm.conf in the
  boss/ops directories to reflect the new LVM paths, etc.

* Fire up the VMs. Ops has to be first, followed by boss.

	sudo xm create ~elabman/ops/xm.conf
	sleep 30
	sudo xm create ~elabman/boss/xm.conf

* It is possible that ops will hang on fixarp, because tmcd is not
  running on boss yet. Log into boss (as elabman) and do:

	sudo testbed-control boot

  which should get ops running.

* named setup does not handle reverse maps smaller then /24 cause of
  the delegation stuff. Needs to be defined as a partial map since
  that is what the upper subnet delegates. But we do not handle this
  in the named config scripts. So I had to edit /etc/namedb/named.conf
  and add this (edit IP of course) to both views. Be sure to delete the
  existing reverse zone (both views) since it is incorrect.

    zone "129/25.242.1.192.in-addr.arpa" in {
    	type master;
    	file "reverse/192.1.242.db";
    };

  Run named_setup. Tail /var/log/messages to look for errors. 

* Now it is time to power on the experimental nodes. If all goes well,
  they will boot up into FreeBSD MFS and be in the hwdown experiment.
  Before we release them, we want to change some settings on the ilo.
  The following will change the Admin password, create an elabman
  user, load its ssh key, change the boot order, etc, etc.

	sudo sh /usr/testbed/etc/initilo.sh

* NOTE: scripted ssh key upload broke in the new iLo, which the script
  above used to take care of. So now we need to upload the root ssh
  key to each node's ilo using the web interface instead. The above
  command happily changed all the passwords so they are now the same
  so that makes it a little easier. Use this key:

	/root/.ssh/id_dsa.pub

  Install the key into BOTH the ELABMAN and ADMINISTRATOR accounts!!!!!

  Now do this:

	/usr/testbed/sbin/initilo.pl -b pc1
	/usr/testbed/sbin/initilo.pl -b pc2
	/usr/testbed/sbin/initilo.pl -b pc3
	/usr/testbed/sbin/initilo.pl -b pc4
	/usr/testbed/sbin/initilo.pl -b pc5

  DAMN YOU HP!

* The above command resets the ilo, so lets play the minute waltz, maybe
  twice.

	http://www.pianoparadise.com/downloadmp3/nocturne.wav

* Now we power on all of the nodes.

	wap power on pc1 pc2 pc3 pc4 pc5

* If the nodes were actually off, it is going to take a couple of minutes
  before we can go on with the next step. Play the waltz a few more times.

* Free all the nodes up and lets hope they reload okay. Okay, lets
  just do one to start with.

	wap nfree emulab-ops hwdown pc1

  If that works and pc1 does indeed go into the free pool, then do the
  rest of the nodes:

  	wap nfree emulab-ops hwdown pc2 pc3 pc4 pc5

* Enable this site in Utah (run this on Utah Emulab boss).

	sudo cacontrol -c boss.XXX.XXX.XXX

* On the new boss, need to reload the bundles:

	sudo /usr/testbed/sbin/protogeni/getcacerts

* Arrange for the VMs to auto starts (on the control node):

	cd /etc/xen/auto/
	sudo ln -s ~elabman/ops/xm.conf 1.ops.conf
	sudo ln -s ~elabman/boss/xm.conf 2.boss.conf

* Next we want to update the firmware on the data plane switch to the
  one that supports openflow. First copy the firmware from Utah to the
  local tftp directory on boss.

	cd /tftpboot
	sudo wget http://www.emulab.net/downloads/K_15_06_5008.swi

* Log into procurve2 using the password in /usr/testbed/etc/switch.pswd
  We then want to make a copy of the current config in case we have to
  revert back.

	5406> show config files
	5406> copy config config1 config config-save
       
* Now load the openflow firmware into the primary flash. First make
  sure the secondary has a copy of the primary. 

	5406> show flash
	5406> copy tftp flash 10.3.1.1 K_15_06_5008.swi

* And if that works, reboot the switch.

	5406> reload

* Wait for the switch to reboot, now lets see if snmpit works:

	boss> wap snmpit -l -l -O

  NOTE: You may see this warning:

	No such VLAN control-hardware in lans table

  No worries, you can ignore it. 

* Telnet to procurve2 (/usr/testbed/etc/switch.pswd):

	5400> config
	5400(config)# openflow
	5400(openFlow)# vlan 1750
	5400(openFlow vlan-1750)# enable
	5400(openFlow vlan-1750)# controller "tcp:10.3.1.7:6633" fail-secure on
	5400(openFlow vlan-1750)# exit
	5400(openFlow)# exit
	5400(config)> write memory

* Add the public IP space, if appropriate. See the site survey response.
  This needs to go into the image baking script.

	boss> wap addvpubaddr 192.1.242.150 192.1.242.179
	boss> wap addvpubaddr 192.1.242.190 192.1.242.250

* Most sites will have the openflow vlan plumbed through to port 24 on
  the data switch. If this is indeed true, then we can go ahead and set
  that up now, so Nick can get started on the FOAM stuff.

	boss> wap addspecialdevice -t interconnect -s 100 XXXX-XXXX
        boss> wap addspecialiface -b 1Gb -s procurve2,1,24 XXXX-XXXX eth0

  where XXXX-XXXX is a pithy name like interconnect-campus. Ask the
  local site admin. If the rack is wired properly 1,24 is correct.
  If the module is in another slot, then this might be different. 

  Now create and share the openflow vlan:

	boss> wap snmpit_test --vlan_tag=1750 -m mesoscale-openflow \
		emulab-ops openflow-vlans XXXX-YYYY:eth0
	boss> wap sharevlan -o emulab-ops,openflow-vlans \
		mesoscale-openflow mesoscale-openflow

* Create some test experiments. Test the ProtoGeni API.

* Create the shared-pool experiment. Use this NS file:

	testbed/install/genirack/shared-exp.ns

  You will need to toggle the lockdown on the Show Experiment page first,
  and then toggle it back after the experiment has swapped in. Be sure to
  test shared node experiments.

* Need to reset the mailing lists to the local admin.

  All of the mailing lists are stored in ops:/etc/mail/lists. Add the local
  admin to the testbed-*.list files. Do not change the defs file, since we
  want the email to go to the local admin *and* Utah.

* Run register_resources

	boss> wap /usr/testbed/sbin/protogeni/register_resources

* Run update_sitevars and addservers to get boss/ops into the DB
  and to turn on arp lockdown.

	boss> wap /usr/testbed/sbin/update_sitevars 
	boss> wap /usr/testbed/sbin/addservers 

---
TODO:

routable ip space when baking the images.
Enable necessary features.

-----
SSH enable on the switches does not work.

ip ssh public-key manager "ssh-rsa AAA ..."
# aaa authentication ssh enable public-key
HP-E2620-24(config)# aaa authentication ssh login public-key 

